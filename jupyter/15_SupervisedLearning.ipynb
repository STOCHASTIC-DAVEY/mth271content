{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(106,127,16)\">Supervised learning </span>\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:rgb(106,127,16)\">May 21, 2020\n",
    "    </span></div>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "*Machine learning*  refers to mathematical and statistical techniques to build  *models of data.* A program is said to *learn* from data when it chooses a model or adapts tunable model parameters  to observed data. In broad strokes, machine learning techniques may be divided as follows:\n",
    "\n",
    "\n",
    "- *Supervised learning*: Models that can predict labels based on labeled training data\n",
    "\n",
    "  - *Classification*: Models that predict labels as two or more discrete categories\n",
    "  - *Regression*: Models that predict continuous labels\n",
    "  \n",
    "- *Unsupervised learning*: Models that identify structure in unlabeled data\n",
    "\n",
    "  - *Clustering*: Models that detect and identify distinct groups in the data\n",
    "  - *Dimensionality reduction*: Models that identify lower-dimensional structure in higher-dimensional data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we  focus on supervised learning. Note the two further subdivisions  mentioned above within the category of supervised learning, and here are two examples within each for further orientation:\n",
    "\n",
    "- Classification example: identify an email as spam or not (discrete label) based on counts of trigger words.\n",
    "- Regression example: predict the arrival time (continuous label) of a streetcar at a station based on past data.\n",
    "\n",
    "We shall further focus on *regression* in this activity. Regression addresses an age-old fitting problem: given a set of data,  find a line (or a curve, or a surface, or a hypersurface in higher dimensions) that approximately fits the data. The equation of the line, in the machine learning language,  is the *model* of the data that has been \"learnt.\"  The model can then \"predict\" the values, i.e., \"labels\" at points not covered by the original data set. Finding equations of curves that pass through a given set of points is the problem of *interpolation*, which goes at least as far back as Newton (1675). The fitting problem in regression, also known at least as far back as Gauss (1809), is a relaxed version of the interpolation problem in that it does not require the curves to pass through the given data, and is generally more suitable to handle noisy data.\n",
    "\n",
    "These days, when machine learning comes at you with the brashness of an overachieving new kid on the block, it is not fashionable to view the subject from the perspective of established mathematical techniques with rich histories. Instead, it has somehow become more fashionable to view machine learning as some sort of new AI miracle. Please do not expect any miracles here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from the linear regression in a form you have  seen previously: given data points $(x_i, f_i)$, $i=0,1, \\ldots, N$, fit a linear equation \n",
    "$$\n",
    "f(x) = a_0 + a_1 x\n",
    "$$\n",
    "to the data, in such a way that the error in the fit \n",
    "$$\n",
    "e = \\sum_{i=0}^N | f(x_i) - f_i|^2 \n",
    "$$\n",
    "is minimized. \n",
    "Since the quantity on the right is a sum of squares, this is called the *least-squares* error.  It is easy to solve this minimization problem. Writing \n",
    "\n",
    "$$\n",
    "Y^\\text{data} =\n",
    "\\begin{bmatrix}\n",
    "f_0 \\\\ \\vdots \\\\ f_N \n",
    "\\end{bmatrix},\n",
    "\\qquad\\quad\n",
    "Y^\\text{fit} = \n",
    "\\begin{bmatrix}\n",
    "f(x_0) \\\\ \\vdots \\\\ f (x_N) \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "1 & x_0  \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_N \n",
    "\\end{bmatrix}\n",
    "}_{X}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "a_0  \\\\  a_1\n",
    "\\end{bmatrix}\n",
    "}_{a}\n",
    "$$\n",
    "the error  $e$ can also be expressed as\n",
    "$\n",
    "e = \\| Y^\\text{fit} - Y^{\\text{data}} \\|^2 = \\| X a  - Y^{\\text{data}} \\|^2\n",
    "= (X a  - Y^{\\text{data}})^t (X a  - Y^{\\text{data}}).\n",
    "$ \n",
    "Now, either from linear algebra, or from calculus, one concludes that $e$ is minimized when \n",
    "\n",
    "$$\n",
    "a = (X^t X)^{-1} X^t Y^\\text{data}.\n",
    "$$\n",
    "\n",
    "This is the  *least-squares solution to the linear regression problem.*\n",
    "\n",
    "In the machine learning language, \n",
    "- $f_i$ are (continuous) \"labels\", \n",
    "- the \"model\" is the linear formula $a_0 + a_1 x$, \n",
    "- the \"labeled training data\" is $(x_i, f_i)$, and \n",
    "- the \"predictions\" are values of $f(x)$ at various $x$-values. \n",
    "\n",
    "Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5 * rng.random(20)\n",
    "f = 3 * x + 5 * rng.random(20)\n",
    "plt.scatter(x, f); plt.xlabel('x'); plt.ylabel('Continuous labels (f)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a linear trend, so let's try to fit a line to it using linear regression formula we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.ones(len(x)), x]).T\n",
    "a = inv(X.T @ X) @ X.T @ f               # Create the \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.linspace(0, 5, num=100) \n",
    "f_predict = a[0]  + a[1] * x_predict     # \"Predict\" using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, f)\n",
    "plt.xlabel('x'); plt.ylabel('Continuous labels (f)');\n",
    "plt.plot(x_predict, f_predict, 'c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a visual representation of the error that is minimized by this line, we can plot line segments (the red thick lines below) whose sum of squared lengths is what we minimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "fp = X @ a\n",
    "plt.scatter(x, f)\n",
    "lc = LineCollection([[(x[i], f[i]), (x[i], fp[i])] \n",
    "                     for i in range(len(x))], color='r', linewidth=4, alpha=0.5)\n",
    "plt.gca().add_collection(lc)\n",
    "plt.xlabel('x'); plt.ylabel('Continuous labels (f)');\n",
    "plt.plot(x_predict, f_predict, 'c'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save there results for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_example = {'data': [x, f], 'model': a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of  linear regression is very similar in higher dimensions.  To fit some given data $f_i$ on $N+1$ points $\\vec{x}_i$, each of which are $m$-dimensional, we express the points in coordinates $\\vec{x}_i = (x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(m)})$.  The model now is\n",
    "\n",
    "$$\n",
    "f(x^{(1)}, x^{(2)}, \\ldots, x^{(m)}) = a_0 + a_1 x^{(1)} + \\cdots + a_m x^{(m)}.\n",
    "$$\n",
    "\n",
    "Exactly the same algebra as before yields the *same solution formula*\n",
    "\n",
    "$$\n",
    "a = (X^t X)^{-1} X^t Y^\\text{data},\n",
    "$$\n",
    "\n",
    "the only difference now being that\n",
    "$$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "a_0 \\\\ \\vdots \\\\ a_m\n",
    "\\end{bmatrix}, \\qquad\\quad\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & x_0^{(1)} &  x_0^{(2)} & \\cdots & x_0^{(m)} \\\\ \n",
    "1 & x_1^{(1)} &  x_1^{(2)} & \\cdots & x_1^{(m)} \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_N^{(1)} &  x_N^{(2)} & \\cdots & x_N^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Here is a 3D example, where we can still attempt to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 5 * rng.random(100)\n",
    "x2 = 5 * rng.random(100)\n",
    "f = 10 - (3*x1 + 2* x2 +  2 * rng.random(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.ones(len(x1)), x1, x2]).T\n",
    "a = np.linalg.inv(X.T @ X) @ X.T @ f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.figure().gca( projection='3d')\n",
    "ax.scatter(x1, x2, f)\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca( projection='3d')\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
    "xx1, xx2 = np.meshgrid(x1, x2)\n",
    "zz = a[0] + a[1] * xx1 + a[2] * xx2\n",
    "ax.plot_wireframe(xx1, xx2, zz, color='c', alpha=0.2)\n",
    "ax.scatter(x1, x2, f); ax.set_title('Modeling data by a plane');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these results for later examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planar_example = {'data': [np.array([x1, x2]).T,  f], 'model': a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting\n",
    "\n",
    "If you know that your data is exponential, you might get better results by fitting with exponentials instead of linear functions. The \"linear\" regression process can be  adapted to use exponentials, or gaussians, or indeed any basis functions you feel are particularly appropriate for your data set. The linearity in \"linear\" regression refers to the linear dependence of the model on the data (and has nothing to do which whether your model $f$ is linear or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving the general formula is done by the same method. Using basis functions $\\phi_j(\\vec x)$, we can  fit  given data $f_i$ on $N+1$ points $\\vec{x}_i$ using the  model\n",
    "$$\n",
    "f(\\vec x ) = \n",
    "a_0 \\phi_0(\\vec x) + a_1 \\phi_1(\\vec x ) + \\cdots + a_m \\phi_m(\\vec x ).\n",
    "$$\n",
    "Again, the previous algebra yields the *same solution formula* \n",
    "\n",
    "$$\n",
    "a = (X^t X)^{-1} X^t Y^\\text{data},\n",
    "$$\n",
    "\n",
    "for the model parameters that provide the minimizer of\n",
    "$$\n",
    "e = \\sum_{i=0}^N | f(\\vec x_i) - f_i|^2. \n",
    "$$\n",
    "The only difference now is that  \n",
    "$$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "a_0 \\\\ \\vdots \\\\ a_m\n",
    "\\end{bmatrix}, \\qquad\\quad\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\vec x_0) & \\phi_1(\\vec x_0)  & \\cdots & \\phi_m(\\vec x_0) \\\\\n",
    "\\phi_0(\\vec x_1) & \\phi_1(\\vec x_1)  & \\cdots & \\phi_m(\\vec x_1) \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\phi_0(\\vec x_N) & \\phi_1(\\vec x_N)  & \\cdots & \\phi_m(\\vec x_N) \n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where we fit a quadratic curve to a simple one-dimensional data set, i.e., here \n",
    "$$\n",
    "f(x) = a_0 + a_1 x + a_2 x^2\n",
    "$$\n",
    "and the $a$'s are found by the above formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5 * rng.random(50)\n",
    "f = 3 * np.exp(x/2) + 2 * rng.random(50)\n",
    "plt.scatter(x, f); plt.xlabel('x'); plt.ylabel('Continuous labels (f)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi0 = np.ones(len(x))\n",
    "phi1 = x\n",
    "phi2 = x**2\n",
    "\n",
    "X = np.array([phi0, phi1, phi2]).T\n",
    "a = np.linalg.inv(X.T @ X) @ X.T @ f   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcurve_predict = np.linspace(0, 5, num=500)\n",
    "phi0 = np.ones(len(xcurve_predict))\n",
    "phi1 = xcurve_predict\n",
    "phi2 = xcurve_predict**2\n",
    "\n",
    "fcurve_predict = a[0] * phi0  + a[1] * phi1  + a[2] * phi2\n",
    "plt.scatter(x, f)\n",
    "plt.xlabel('x'); plt.ylabel('Continuous labels (f)');\n",
    "plt.plot(xcurve_predict, fcurve_predict, 'c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had attempted to fit a  straight-line through the data, then we would not have gotten such a close fit. Another way of saying this in the prevalent terminology is that linear features *underfit* this data, or that the linear model has *high bias.* Saving this example also for later,  we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_example = {'data': [x, f], 'model': a, 'type': 'quadratic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The module `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the regression computations we did above can be done using the module `scikit-learn`. Of course, the formulas above were simple and easy to implement. The power of `scikit-learn` is not in its linear regression implementation, but rather, in the vast range of many other  ready-made facilities it provides under a unified user interface. When faced with a package that attempts to do so many things, it's a good entry strategy to confirm that it behaves as we expect in situations we know. This was our purpose in using the simple regression as an entry point into `scikit-learn`.\n",
    "\n",
    "\n",
    "Let's check if our  first-principles computation of regression solutions match what `scikit-learn`'s answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit data to this `model` using the `fit` method. Let's fit the same data we used in the first example of this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, f = linear_example['data']   # Recall the saved data from the first example\n",
    "model.fit(x[:, np.newaxis], f)  # Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 5, num=100)\n",
    "ffit = model.predict(xfit[:, np.newaxis])   # Prediction step\n",
    "plt.scatter(x, f); \n",
    "plt.xlabel('x'); plt.ylabel('Continuous labels (f)');\n",
    "plt.plot(xfit, ffit, 'c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we seem to be getting the same result. We can confirm the results are exactly the same by digging into the solution components within the `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same as the values we solved for previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_example['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting process in `scikit-learn` is similar in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x12, f = planar_example['data']\n",
    "model.fit(x12, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our previously computed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planar_example['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, regression for curve fitting is also possible in `scikit-learn`. The difference now is that here you begin to see how things would get easier if you learn their language. \n",
    "\n",
    "Scikit-learn uses the word *estimator* for models in machine learning. In the module, estimators are  python objects that implement the methods `fit` and `predict`. We have already seen both in the context of the above regression examples. Additional terminology we should know include *transformer* (objects which can map/transform data into some other form) and *pipeline* (a sequence of transformers followed by an estimator). \n",
    "\n",
    "\n",
    "The term *feature* is used for many things:  data attributes, elements of a data row, columns of a data array,  the range of a function mapping some data values, etc. When we decide that a data set should be fitted with some basis functions, linear or not, scikit-learn also uses the word feature to refer to the basis.  In fact, the process of selecting such basis functions is an example of *feature engineering*. More generally, feature engineering is any process by which raw information (data)  is converted into numbers or other mathematical objects, things inside a *feature matrix.*  Tidy data in a feature matrix has each *variable/feature in a column* and each *observation/sample in a row*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using polynomial *feature*s, we create quadratic basis functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = PolynomialFeatures(3, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a *transform*(er):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([5, 7, 9])[:, np.newaxis]\n",
    "q.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the feature `q` performed the data transformation \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\ x_1  \\\\ \\vdots \\\\ x_N\n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\vec x_0) & \\phi_1(\\vec x_0) & \\cdots & \\phi_m(\\vec x_0) \\\\\n",
    "\\phi_0(\\vec x_1) & \\phi_1(\\vec x_1)  & \\cdots & \\phi_m(\\vec x_1) \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\phi_0(\\vec x_N) & \\phi_1(\\vec x_N)  & \\cdots & \\phi_m(\\vec x_N) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "for $\\{ \\phi_i(x)\\} = \\{ x, x^2, x^3\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve fitting\n",
    "\n",
    "The point of view taken by scikit-learn for curve fitting is that it is a process obtained by applying the  (common) linear regression formula after applying the above transformer. Therefore, one would implement it using a pipeline object where this transformer is chained to a linear regression estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the data from the curve-fitting example we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = curve_example['data']  # load data from the prior example\n",
    "# make model/pipeline and fit the data to it:\n",
    "quadratic_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "quadratic_model.fit(x[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = quadratic_model.predict(xfit[:, np.newaxis])\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cross-check that the model parameters are  exactly the same after fitting by examining the `LinearRegression` object in the quadratic model pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_model.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_model.named_steps['linearregression'].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_model.named_steps['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These match our previously computed results for quadratic fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_example['model']  # previously saved results from first principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discussion should have built some confidence in scikit-learn's abilities under the hood. There is plenty of material online, and in your [textbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.00-machine-learning.html), on how to use scikit-learn and other machine learning packages, which I encourage you to read (especially the textbook's discussion of important pitfalls such as overfitting).  However, it is quite a bit harder to find out the precise mathematical algorithm each facility uses:  the documentation is designed for quick users in a rapidly changing field, and therefore understandably does not get into the math. I realize this may not be comforting to you as students of mathematics, so my focus here and in the next few lectures is to connect these software tools with the mathematics you know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:rgb(106,127,16); font-size:8pt\">These materials were created by</span> [<span style=\"color:rgb(106,127,16); font-size:8pt\">Jay Gopalakrishnan</span>](http://web.pdx.edu/~gjay/) <span style=\"color:rgb(106,127,16); font-size:8pt\">for a sophomore course (MTH 271) offered during the Spring 2020 quarter at Portland State University, and are made available under the</span> [<span style=\"color:rgb(106,127,16) ; font-size:8pt\">CC-BY-SA license</span>](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
