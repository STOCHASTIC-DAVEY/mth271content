{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(106,127,16)\">The Singular Value Decomposition</span>\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:rgb(106,127,16)\">April 27, 2020\n",
    "    </span></div>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the early and natural ideas in software development for scientific computation was the idea of packaging linear algebra software around robust implementations of  matrix factorizations, divested from specific applications. This enabled durable linear algebra tools like [LAPACK](http://www.netlib.org/lapack/) to be developed with a stable interface, usable across  numerous application domains. Commercial software like matlab, as well as open-source software like octave, numpy and scipy, all take full advantage of these developments.   What does this mean for you as a data scientist?  When you are faced with a specific computational task, if you are able to reformulate your task using off-the-shelf implementations of matrix factorizations, then you might already be half-way to finishing your task.\n",
    "\n",
    "\n",
    "You already know about some matrix factorizations. In your introductory linear algebra prerequisite course, you learnt about eigenvalues and eigenvectors. The computation of eigenvalues and eigenvectors is indeed the computation of a matrix factorization. This factorization is called a **diagonalization** or an **eigenvalue decomposition** of an $n \\times n$ matrix $A$ and it takes the form \n",
    "$$\n",
    "A = X D X^{-1}\n",
    "$$\n",
    "where $D$ is a diagonal matrix and $X$ is an invertible matrix, both of the same size as $A$. The eigenvalues of $A$ are found in the diagonal entries of $D$ and the eigenvectors are columns of $X$, as can be see by rewriting the factorization as\n",
    "$$\n",
    "A X = X D. \n",
    "$$\n",
    "I think you have already seen the importance of eigenvalues in varied applications, so I'll not stress it further. \n",
    "\n",
    "\n",
    "Another important factorization is the SVD, or the **singular value decomposition**, which often does not get the emphasis it deserves in lower division courses.  In some ways the SVD is even more important that a diagonalization. This is because not all matrices have a diagonalization. In contrast, using basic real analysis results, one can prove that any matrix has an SVD.  We shall see that from an SVD, one can read off the important properties of a matrix and  easily construct compressed approximations of the matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is a factorization of an $m \\times n$ matrix $A$ of the form \n",
    "$$\n",
    "A = U \\Sigma V^*\n",
    "$$\n",
    "where $\\Sigma$ is am $m \\times n$ diagonal matrix, and $U$ and $V$ are unitary matrices of sized  $m \\times m$ and $n \\times n$, respectively. (Recall that a square matrix $Q$ is called unitary if its inverse equals  $Q^*$, the conjugate transpose of $Q$.) The diagonal entries of $\\Sigma$ are non-negative and positive ones are called the **singular values** of $A$. It is a convention to list the singular values in non-increasing order along the diagonal. The columns of $U$ and $V$ are called the left and right **singular vectors**, respectively. \n",
    "\n",
    "\n",
    "Here is how we compute SVD using `scipy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(4, 5) + 1j * np.random.rand(4, 5)\n",
    "u, s, vh = svd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u @ u.T.conjugate()        # u is unitary. Its columns are left singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh @ vh.T.conjugate()      # Rows of vh are right singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s     # Only the diagonal entries of Sigma are returned in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algebra of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *outer product* of an $x \\in \\mathbb{R}^m$ and $y \\in \\mathbb{R}^n$, is the \n",
    "$m \\times n$ matrix $x y^*$ (which being the product of $m \\times 1$ and $1 \\times n$ matrices, is of shape $m \\times n$). Reversing the order of $x$ and $y^*$ in the product, we of course get the familiar *inner product*, which is a $1 \\times 1$ matrix, or a scalar. \n",
    "\n",
    "Although the outer product is an $m \\times n$ matrix, with $mn$ entries, it only takes $m+n$ entries to completely specify it (namely the entries of $x$ vector and the $y$ vector). Note that the columns of the outer product $x y^*$ are \n",
    "$$\n",
    "\\bar{y}_1 x,\\; \\bar{y}_2 x,  \\ldots, \\;\\bar{y}_n x.\n",
    "$$\n",
    "In other words all columns are scalar multiples of the same vector $x$. Therefore, whenever $x$ is a nontrivial vector, the dimension of the *range* (or the *column space*) of the matrix is 1. Recall from your linear algebra prerequisite that this dimension is what we call *rank*. All outer products are of unit rank (unless one of the vectors is trivial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very useful way to think of the SVD is to expand the factorization as follows. Naming the columns of $U$ and $V$ as $u_i$ and $v_j$, we have\n",
    "$$\n",
    "A = U \\Sigma V^* = [u_1, \\ldots, u_m] \n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 &          &  \\\\\n",
    "         & \\sigma_2   \\\\ \n",
    "         &          & \\ddots\n",
    "\\end{bmatrix}\n",
    "[v_1, \\ldots, v_n]^*\n",
    "= \n",
    "\\sum_{l=1}^{\\min(m, n)} \\sigma_l \\; u_l v_l^*.\n",
    "$$\n",
    "Thus the SVD can be viewed as a sum of unit rank outer products. Each summand increases rank (if the corresponding singular value is nonzero) until the rank of $A$ is reached. Let's see this in action for a small matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(4, 5)\n",
    "u, s, vh = svd(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy's broadcasting rules do not make it easy to make the  outer product $u_l v_l^*$ simply. You have to place a `newaxis` in the right places to make it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[0, :, np.newaxis] @ vh[np.newaxis, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternately, you can use `np.outer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(u[0, :], vh[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the sum $\\sum_l \\sigma_l (u_l v_l^*)$, we find that it is equal to `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.zeros_like(a)\n",
    "for i in range(4):\n",
    "    ar += np.outer(u[:, i], s[i] * vh[i, :])\n",
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factors of the SVD tell us all the important properties of the a matrix immediately. If you enjoy linear algebra, you should try to prove the following simple result yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem.** Suppose $A = U \\Sigma V^*$ is the SVD of an $m \\times n$ matrix $A$. Then the following statements hold.\n",
    "1. The rank of $A$ is the number of nonzero singular values. \n",
    "2. A basis for the range (column space) of $A$ is $\\{u_1, u_2, \\ldots, u_r\\}$.\n",
    "3. A basis for the null space (kernel) of $A$ is $\\{ v_{r+1}, \\ldots, v_{n-1}, v_n\\}$.\n",
    "4. The singular values of $A$ are non-negative square roots of eigenvalues of $A^*A$.\n",
    "\n",
    "Notice how the rank-nullity theorem (something you may have been tortured with in your first linear algebra course) follows as a trivial consequence of items (2) and (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The geometry of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometry of any linear operator in $\\mathbb{R}^n$ is easy to describe: application of a matrix transforms (hyper)spheres to (hyper)ellipses - if you did not know this, you will momentarily see this from the code below. Unitary operators are special in that they are coordinate changes that maps (hyper)spheres to (hyper)spheres. In general unitary operators don't change angles - they include operations like rotation and reflection in higher dimensions. \n",
    "\n",
    "The presence of unitary factors in the SVD is significant. The SVD provides a geometrical decomposition of a linear operator  into factors $U$ and $V^*$ that  do not change the shapes, and a factor $\\Sigma$ that stretches axial directions (so that the shape change is transparent). Let us see this in action for a $ 2 \\times 2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.1, 0.5], [0.4, 0.8]])\n",
    "u, s, vh = svd(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the geometry gets transformed (squashed) by the linear operator (matrix) `a`, we first plot the unit circle and the parts of the $x$ and $y$ axis within it. Then, we track how these points are mapped by `a`, as well as by each of the components of the SVD of `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show(c):\n",
    "    plt.plot(c[0, :], c[1, :])\n",
    "    plt.axis('image');\n",
    "\n",
    "# plot the unit circle and axis segments:\n",
    "\n",
    "t = np.linspace(0, 3.5 * np.pi , num=300)\n",
    "l = np.linspace(-1, 1, num=10)\n",
    "z = np.zeros_like(l)\n",
    "c = np.array([np.concatenate([l, np.cos(t), z]), np.concatenate([z, np.sin(t), l])])\n",
    "show(c)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what `a` does to this geometry: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(a @ c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see this transformation as a composition of the following three geometrical operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(vh @ c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(np.diag(s) @ c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(u @ c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you compose these operations, you indeed get the transformation generated by `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(u @ np.diag(s) @ vh @ c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low rank approximation\n",
    "\n",
    "\n",
    "There are many ways of expressing a matrix as a sum of low rank matrices, e.g., \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "a & 0 \\\\ 0 & 0  \n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix}\n",
    "0 & b \\\\ 0 & 0 \n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\ c & 0 \n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\ 0 & d \n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Each of the matrices on the right can have rank at most one.\n",
    "\n",
    "As we have already seen, the SVD also expresses $A$ as a sum of rank-one outer products. However, the way the SVD does this, is special in that a low-rank minimizer can be read off the SVD, as described in the following (Eckart-Young-Mirksy) theorem. Here we compare matrices of the same size using the *Frobenius* norm\n",
    "$$\n",
    "\\| A \\|_F = \\bigg( \\sum_{i, j} |A_{ij}|^2 \\bigg)^{1/2}.\n",
    "$$\n",
    "The theorem answers the following question: how close can we get to $A$ using matrices whose rank is much lower than the rank of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem.** Suppose $A$ be an $m \\times n$ matrix (complex or real). For any $0 \\le \\ell \\le r = \\text{rank}(A)$, define the matrix \n",
    "$$\n",
    "A_\\ell = \\sum_{j=1}^{\\ell} \\sigma_j u_j v_j^*,\n",
    "$$\n",
    "using the singular values $\\sigma_j$ and the left and right singular vectors $u_j, v_j$ of $A$,  i.e., $A_\\ell$ is the sum of the first $\\ell$ terms of the SVD when written as a sum of outer products.  Then, the minimum of $ \\| A - B \\|_F$ over all $m \\times n$ matrices $B$ of rank not more than $\\ell$ is attained by $\\| A - A_\\ell \\|_F$ and the minimum is \n",
    "$(\\sigma_{\\ell+1}^2 + \\cdots + \\sigma_r^2)^{1/2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix approximation result is perhaps easily visualized using images. Think of an image as a matrix. Using `matplotlib`'s `imread` we can load PNG images. Here is an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = plt.imread('../figs/GeoLea.png')\n",
    "cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3-dimensional array because images are represented using RGBA values at each pixel (red, green, blue and an alpha value for transparency). However this image of my cats is actually a black and white image, so all RGB values are the same, as can be verified immediately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(cats[..., 0] - cats[..., 2], 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above I have squeezed in two things that may be new for you: the use of the [ellipsis](https://docs.python.org/dev/library/constants.html#Ellipsis)  to leave the dimension of a numpy slice unspecified, and the way to compute the Frobenius norm in numpy. Restricting to the first of the three identical channels, we continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cats[..., 0]  \n",
    "plt.imshow(c, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the SVD of this 1040 x 758 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = svd(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a sharp drop in the magnitude of the singular values. This is a good indication that the later summands in the SVD representation of $A$,  \n",
    "$$\n",
    "A = \\sum_{j=1}^{\\min(m, n)} \\sigma_j u_j v_j^*\n",
    "$$\n",
    "are adding much less to $A$ than the first few summands. Therefore, we should be able to represent the same $A$ using the first few outer products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 20 approximation of the cats:\n",
    "\n",
    "l = 20;   cl = u[:, :l] @ np.diag(s[:l]) @ vh[:l, :]\n",
    "plt.imshow(cl, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 50 approximation of the cats:\n",
    "\n",
    "l = 50;   cl = u[:, :l] @ np.diag(s[:l]) @ vh[:l, :]\n",
    "plt.imshow(cl, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you increase the rank `l` to 100, you will find that the result is visually indistinguishable from the original. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the theorem, let us note that the theorem also gives one the ability to specify an error tolerance and let that dictate the choice of the rank $\\ell$. As an example, suppose I declare I want a low-rank approximation within the following relative error in Frobenius norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error = 1.e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find the needed $\\ell$ using an aggregation and masking as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s**2\n",
    "total = np.sum(s2)\n",
    "diff = np.sqrt((total - np.add.accumulate(s2)) / total)\n",
    "l = np.argmax(diff < relative_error) + 1\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then here is the needed low rank approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cl = u[:, :l] @ np.diag(s[:l]) @ vh[:l, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the error is indeed less than the prescribed error tolerance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(c - cl, 'fro') / np.linalg.norm(c, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the low rank approximation does give some image compression. The number of entries needed to store a rank $\\ell$ approximation `cl` of an $m \\times n$ matrix  is $m \\ell + \\ell + \\ell n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.shape[0] * l + l + l * vh.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, to store the original image (single channel) we would need to minimally store $mn$ numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape[0] * c.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly, we have some compression. However, for image compression, there are better algorithms. \n",
    "\n",
    "\n",
    "The  utility of the low-rank approximation technique using the SVD is not really in image compression, but rather in other applications that leverage the operator. Being an compressed approximation of an *operator* (i.e., a matrix) is much more than just a compressed visual image. For example, one can not only reduce the storage for a matrix, but also reduce the time to apply the operator to a vector using a low-rank approximation. Because the SVD tells us the essentials of an operator in lower dimensional spaces, it continues to find new applications in many modern emerging concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:rgb(106,127,16); font-size:8pt\">These materials were created by</span> [<span style=\"color:rgb(106,127,16); font-size:8pt\">Jay Gopalakrishnan</span>](http://web.pdx.edu/~gjay/) <span style=\"color:rgb(106,127,16); font-size:8pt\">for a sophomore course (MTH 271) offered during the Spring 2020 quarter at Portland State University, and are made available under the</span> [<span style=\"color:rgb(106,127,16) ; font-size:8pt\">CC-BY-SA license</span>](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
